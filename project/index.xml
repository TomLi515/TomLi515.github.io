<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Academic</title>
    <link>https://TomLi515.github.io/project/</link>
      <atom:link href="https://TomLi515.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 27 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://TomLi515.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://TomLi515.github.io/project/</link>
    </image>
    
    <item>
      <title>EAN: An Efficient Attention Module Guided by Normalization for Deep Neural Networks</title>
      <link>https://TomLi515.github.io/project/research2/</link>
      <pubDate>Sun, 27 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://TomLi515.github.io/project/research2/</guid>
      <description>&lt;p&gt;&lt;em&gt;AAAI 2024 Accepted, 2nd Author, Supervised by Prof. Jiafeng Li, East China Normal University&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Innovatively propose an Efficient Attention module guided by Normalization (EAN), a plug-and-play module investigating the intrinsic relationship between feature normalization and attention mechanisms to enhance model efficiency and accuracy across various visual tasks.&lt;/li&gt;
&lt;li&gt;Integrated EAN with an Attention Generation (AG) unit to derive attention weights using parameter-efficient normalization and guide the network to capture relevant semantic responses while suppressing irrelevant ones. The AG unit harnesses the strengths of normalization and attention and combines them into a unified module to enhance feature representation.&lt;/li&gt;
&lt;li&gt;Successfully achieved exceptional performance of the EAN module across multiple disciplines, including Image Classification and Object Detection. Particularly in Object Detection, EAN outperformed the original ResNet50 and MobileNeXt by 5.5% and 5.4% in the MS COCO dataset, validating the superior accuracy and convergence of our EAN compared to state-of-the-art methods.&lt;/li&gt;
&lt;li&gt;Conclusion: In this paper, we explore the intrinsic relationship between two widely used techniques for enhancing models: feature normalization and attention. Further, we propose an Efficient  Attention module guided by Normalization, named EAN. EAN incorporates an AG unit to derive attention weights using parameter-efficient normalization and guide the network to capture relevant semantic responses while suppressing irrelevant ones. The AG unit harnesses the strengths of normalization and attention and combines them into a unified module to enhance feature representation. EAN is also a plug-and-play module. Extensive experiments on multiple benchmark datasets validate the superior accuracy and convergence of our EAN compared to state-of-the-art methods.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GNN-Based Multi-Turn Dialogue Representation for Retrieval Augmented Generation</title>
      <link>https://TomLi515.github.io/project/research1/</link>
      <pubDate>Sun, 27 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://TomLi515.github.io/project/research1/</guid>
      <description>&lt;p&gt;&lt;em&gt;Planned Submission to ACL 2024, 1st Author, Supervised by Prof. Xiaofan Zhang, Shanghai AI Lab&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identified a new research task in the information retrieval part of RAG, filling the existing gap by focusing on utilizing multi-turn dialogues as queries to effectively search for the best document in a specific database.&lt;/li&gt;
&lt;li&gt;Built a new dataset for the task: collected medical multi-turn dialogues and retrieved medical docs with them from the lab&amp;rsquo;s medical database, integrating medical InternLM (20B) to rank docs by model perplexity.&lt;/li&gt;
&lt;li&gt;Pioneered a Graph Convolutional Network (GCN) approach and designed an innovative graph structure: dialogue turns as nodes and syntactic trees (N-LTP) as edges, capturing key information from the dialogues. Optimized the web search vector and calculated similarity between enhanced vector and the vectors of docs derived from BERT.&lt;/li&gt;
&lt;li&gt;Achieved an impressive MRR at 0.71, and NDCG at 0.73 as baseline based on 7,500 manually annotated dialogues, showing the model&amp;rsquo;s reliable accuracy in retrieving the most suitable document from the database.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NANER: Instance-Aware Named Entity Recognition (NER)</title>
      <link>https://TomLi515.github.io/project/external-project/</link>
      <pubDate>Sun, 27 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://TomLi515.github.io/project/external-project/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Proposed NANER, an NER model that utilized instance-based prompt learning to resolve issues related to category ambiguity and the complexity of obtaining high-quality descriptions.&lt;/li&gt;
&lt;li&gt;Employed an instance-based span model named NASpan within NANER, constructing spans with complete tokens, guided by specific entity instances sampled from training sets or online sources like Wikipedia.&lt;/li&gt;
&lt;li&gt;Verified the robustness of NANER by achieving state-of-the-art F1 improvements on datasets such as ACE04, ACE05, and GENIA. NANER also excelled in domain transfer tasks through zero &amp;amp; few-shot learning, enhancing F1 of 11.13% on CoNLL03 and 8.59% on Wnut17 compared to description-based zero-shot benchmarks.&lt;/li&gt;
&lt;li&gt;Conclusion: In this paper, we proposed a prompt learning method and designed an instance-based span model for named entity recognition. We sample instances from training sets or online sources and use instances for enhanced span construction which contains complete span information. Experimental results and empirical analysis demonstrate that NANER achieves SOTA performance under both supervised and domain transfer settings, which verifies its effectiveness and generalization ability. Besides, based on instance learning, we proposed a fast and effective ensemble method called instance ensemble, which saves many time and computation resources compared with the traditional model ensemble. In future work, we plan to explore how to automatically obtain instances, identify entities in parallel, and extend instance learning to handle more NLP downstream tasks.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
